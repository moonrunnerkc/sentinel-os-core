llm:
  backend: llama-cpp
  model_path: data/models/llama-2-7b.Q4_K_M.gguf
  temperature: 0.0
  seed: 42
  gpu_layers: 0
  min_vram_gb: 4.0

# ollama settings for chatbot
ollama:
  enabled: true
  base_url: "http://localhost:11434"
  model: "llama3.2"
  temperature: 0.0
  seed: 42
  max_tokens: 512
  context_size: 4096
  timeout: 60
  num_thread: 4

# chatbot settings
chatbot:
  enabled: true
  auto_save: true
  save_interval: 5
  max_history_turns: 10
  enable_chain_of_thought: true
  enable_belief_extraction: true
  enable_contradiction_check: true
  dp_noise_epsilon: 0.1

performance:
  # null = no limit, integer = soft limit with warning
  max_beliefs: null
  max_episodes: null
  soft_limit_warning_pct: 80
  page_size: 1000
  cache_size: 1000
  enable_compression: false

features:
  # neuromorphic mode requires brian2 - will fail if unavailable
  neuromorphic_mode: false
  # zk proofs - pedersen commitments and schnorr proofs
  zk_proofs: true
  # homomorphic encryption requires tenseal - hard fail if unavailable
  homomorphic_encryption: false

# crypto settings
crypto:
  commitment_seed: 42

# verification settings
verification:
  enabled: true
  check_invariants_on_transition: true
  property_test_iterations: 50

# privacy settings
privacy:
  total_epsilon: 1.0
  total_delta: 1.0e-5
  composition_mode: basic

# isolation settings
isolation:
  level: python
  timeout_seconds: 30
  use_firejail: false
  network_enabled: false

# meta-evolution settings
meta_evolution:
  enabled: false
  max_generations: 100
  convergence_threshold: 0.001
  seed: 42
  mutation_scale: 0.1
  bounds:
    epsilon: [0.01, 1.0]
    decay_rate: [0.001, 0.1]
    confidence_threshold: [0.1, 0.9]
    learning_rate: [0.001, 0.1]

# world model settings
world_model:
  enabled: false
  type: simple
  decay_rate: 0.01
  regeneration_rate: 0.005
  noise_scale: 0.01
  seed: 42
